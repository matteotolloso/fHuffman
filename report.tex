\documentclass[12pt, letterpaper]{article}  % it must be at least 11 pt or more if you like (here we used 12 for clarity of the demo), and change article with report
\usepackage[letterpaper, top=3.71cm, bottom=3.20cm, left=2.86cm, right=2.86cm]{geometry}
%top = 2.44 (header in doc) + 1.27
% bottom 1.42 (footer in doc) + 1.78
\usepackage{graphicx}
\usepackage{array}
\usepackage{placeins}
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\usepackage{hyperref}
\usepackage{verbatim}
% Square around link (to decide)
\hypersetup{hidelinks}
% Square around link (to decide)
\usepackage{color}
\usepackage{tabularx}
\usepackage{threeparttable}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{longtable}
\usepackage{subcaption} % for subfigure environments
\usepackage{booktabs}
\usepackage{nameref}


\title{\vspace{2cm}\textbf{Parallel implementation of Huffman Code using native C++ threads and FastFlow library} \\
        \bigskip
        \Large{
            \medskip
            Parallel and Distributed Systems: Paradigms and Models \\
            \medskip
            University of Pisa \\
            \medskip
            Project Report\\
            \medskip
        }
}
\medskip

\author{
  {Matteo Tolloso}\\
  \texttt{ \scriptsize{m.tolloso@studenti.unipi.it}}\\
  \texttt{\scriptsize{Roll number: 598067}} \\
  %\scriptsize{Artificial Intelligence curriculum}
}

\begin{document}
\nocite{*}
\date{September 1, 2023}
\maketitle

%/////////////////////////////////////
\newpage

\section{Introduction}
The Huffman code is an efficient lossless compression code based on the probability of each character.

To build the optimal code for a specific text we have to:
\begin{enumerate}
    \item count the number of occurences of each character in the text;
    \item build the binary tree that represents the code;
    \item encode the file.
\end{enumerate}

\section{Theoretical analysis}

We are facing a problem that can be divided in three stages. In particular it is a \textit{data parallel} task, 
since we have all input available at the beginning of the computation.

\subsection{Counting the number of occurences}
As stated above, the first stage is a counting one. This is clearly a \textit{map-reduce} operation.

\paragraph*{Map}
The \textit{map} part can be execute in parallel dividing the file into chunks, the workers count the occurences 
in a chunk of the file. The asymptotic sequential complexity of this part is $\theta(m)$ where $m$ is the number of characters in the file.

This operation has to deal with the disk. Under the following assumptions:
\begin{itemize}
    \item the reading of a chunk from  the disk is slower than the counting operation of that chunk;
    \item the readings are sequential;
\end{itemize}
it seems useless to parallelize this phase, since the disk is the bottleneck of the operation.
The minimum completion time could be achived with two workers in pipeline, one reading and one counting.

Nevertheless, the tests that i did mapping the file in main memory and reading it with multiple threads, showed that the parallelization
actually improves the performance. This is probably due to how the SSD works and the caching syestems. Quantitative results 
are shown in section \ref{sec:results} .

\paragraph*{Reduce}
The \textit{reduce} operation can again be executed in parallel, this 
time each reducer takes a subset of the alphabet and sums the occurences of each character in that subset. The sequential
asymptotic complexity is constant since depends only of he number of different character. However, from a parallel point of view, this operation 
has a complexity that depeds on the number of chunks the file was divided into in the \textit{map} phase (that can be different from the number of
mapper workers), because the reducers have to sum the assigned characters along all the chunks. So if $c$ is the number of chunks and $A$ the number
of different symbols (128), the complexity of the \textit{reduce} phase is $\theta(c \times A)$.

\subsection{Building the binary tree}
The second stage is the building of the binary tree. This is a more difficult operation to parallelize since 
most of the operations are sequential. Furthermore, the complexity of this stage is $\theta(A \times log (A))$  where 
$A$ is the number of differtent symbols (128), so basically it is a constant in our case.

\subsection{Encoding the file}
The last stage is the encoding of the file. This is a \textit{map} operation, since each character has to be replaced with its code.
Unfortunately, the lenght of the final text can only be known after each character has been encoded (because the encoding of each character
has a different lenght), so the actual writing needs a step of syncronization. The sequential complexity of this stage is $\theta(m)$ where $m$ is 
the number of characters in the file.

We can make a similar reasoning as the one made for the \textit{map} phase of the counting stage. The disk could be the bottleneck of the operation.
If we divide the file in chunks and we encode in parallel each of them, we end up with a set of encoded chunks that have to be sorted and summed to 
get the total lenght before writing (if we want to write them in parallel). This operation has a complexity of $\theta(c \times log(c))$ where $c$ is the number of chunks.


\section{Implementation}


\section{Results \label{sec:results}}



\newpage \FloatBarrier
\bibliographystyle{plain}
\bibliography{bibliography}


\end{document}
